{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/02735/edwardk/.theanorc\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.theanorc\n",
    "[global]\n",
    "device=gpu\n",
    "floatX=float32\n",
    "\n",
    "[blas]\n",
    "ldflags=-lopenblas\n",
    "\n",
    "[cuda]\n",
    "root=/opt/apps/cuda/7.0\n",
    "\n",
    "[nvcc]\n",
    "fastmath=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cPickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import squared_error, categorical_crossentropy\n",
    "from lasagne.nonlinearities import softmax\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import BatchIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (100, 5, 48, 48), X.min = 15.2576580048, X.max = 25.4738693237\n",
      "y.shape = (100,), y.min = -1.0, y.max = 1.0\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"/work/02735/edwardk/classify/data/training_images.npy\", mmap_mode=\"r\")\n",
    "print(\"X.shape = {}, X.min = {}, X.max = {}\".format(X.shape, X.min(), X.max()))\n",
    "\n",
    "y = np.load(\"/work/02735/edwardk/classify/data/training_labels.npy\")\n",
    "print(\"y.shape = {}, y.min = {}, y.max = {}\".format(y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (100, 5, 48, 48), X.min = 0.0, X.max = 1.0\n",
      "y.shape = (100,), y.min = 0.0, y.max = 1.0\n"
     ]
    }
   ],
   "source": [
    "def renormalize(array):\n",
    "    return (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "X, y = map(renormalize, (X, y))\n",
    "\n",
    "print(\"X.shape = {}, X.min = {}, X.max = {}\".format(X.shape, X.min(), X.max()))\n",
    "print(\"y.shape = {}, y.min = {}, y.max = {}\".format(y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_PCA(array):\n",
    "\n",
    "    nimages0, nchannels0, height0, width0 = array.shape\n",
    "    rolled = np.transpose(array, (0, 2, 3, 1))\n",
    "    # transpose from N x channels x height x width  to  N x height x width x channels\n",
    "    nimages1, height1, width1, nchannels1 = rolled.shape\n",
    "    # check shapes\n",
    "    assert nimages0 == nimages1\n",
    "    assert nchannels0 == nchannels1\n",
    "    assert height0 == height1\n",
    "    assert width0 == width1\n",
    "    # flatten\n",
    "    reshaped = rolled.reshape(nimages1 * height1 * width1, nchannels1)\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(reshaped)\n",
    "    \n",
    "    cov = pca.get_covariance()\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "    \n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AugmentedBatchIterator(BatchIterator):\n",
    "    \n",
    "    def __init__(self, batch_size, crop_size=8):\n",
    "        super(AugmentedBatchIterator, self).__init__(batch_size)\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "\n",
    "        Xb, yb = super(AugmentedBatchIterator, self).transform(Xb, yb)\n",
    "        batch_size, nchannels, width, height = Xb.shape\n",
    "        \n",
    "        eigenvalues, eigenvectors = compute_PCA(Xb)\n",
    "\n",
    "        # Flip half of the images horizontally at random\n",
    "        indices = np.random.choice(batch_size, batch_size // 2, replace=False)        \n",
    "        Xb[indices] = Xb[indices, :, :, ::-1]\n",
    "\n",
    "        # Crop images\n",
    "        X_new = np.zeros(\n",
    "            (batch_size, nchannels, width - self.crop_size, height - self.crop_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        if yb is not None:\n",
    "            y_new = yb.astype(np.int32)\n",
    "        else:\n",
    "            y_new = None\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Choose x, y pixel posiitions at random\n",
    "            px, py = np.random.choice(self.crop_size, size=2)\n",
    "                \n",
    "            sx = slice(px, px + width - self.crop_size)\n",
    "            sy = slice(py, py + height - self.crop_size)\n",
    "            \n",
    "            # Rotate 0, 90, 180, or 270 degrees at random\n",
    "            nrotate = np.random.choice(4)\n",
    "            \n",
    "            # add random color perturbation\n",
    "            alpha = np.random.normal(loc=0.0, scale=0.5, size=5)\n",
    "            noise = np.dot(eigenvectors, np.transpose(alpha * eigenvalues))\n",
    "            \n",
    "            for j in range(nchannels):\n",
    "                X_new[i, j] = np.rot90(Xb[i, j, sx, sy] + noise[j], k=nrotate)\n",
    "                \n",
    "        return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SaveParams(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if train_history[-1][\"valid_loss_best\"]:\n",
    "            nn.save_params_to(\"{}.params\".format(self.name))\n",
    "            with open(\"{}.history\".format(self.name), \"w\") as f:\n",
    "                pickle.dump(train_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `verbose` parameter to `2`.\n",
    "\n",
    "# 4 dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net39 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "\n",
    "        ('conv11', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('dropout1', layers.DropoutLayer),\n",
    "\n",
    "        ('conv21', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('dropout2', layers.DropoutLayer),\n",
    "\n",
    "        ('conv31', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('dropout3', layers.DropoutLayer),\n",
    "\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('dropout4', layers.DropoutLayer),\n",
    "        \n",
    "        ('hidden5', layers.DenseLayer),\n",
    "\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 5, 40, 40),\n",
    "    \n",
    "    conv11_num_filters=32, conv11_filter_size=(5, 5), \n",
    "    pool1_pool_size=(2, 2),\n",
    "    dropout1_p=0.1,\n",
    "\n",
    "    conv21_num_filters=64, conv21_filter_size=(3, 3),\n",
    "    pool2_pool_size=(2, 2),\n",
    "    dropout2_p=0.2,\n",
    "\n",
    "    conv31_num_filters=128, conv31_filter_size=(3, 3),\n",
    "    pool3_pool_size=(2, 2),\n",
    "    dropout3_p=0.4,\n",
    "\n",
    "    hidden4_num_units=2048,\n",
    "    \n",
    "    dropout4_p=0.5,\n",
    "    hidden5_num_units=2048,\n",
    "\n",
    "    output_num_units=2,\n",
    "    output_nonlinearity=softmax,\n",
    "\n",
    "    update_learning_rate=0.0001,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    objective_loss_function=categorical_crossentropy,\n",
    "    regression=False,\n",
    "    max_epochs=3000,\n",
    "    batch_iterator_train=AugmentedBatchIterator(batch_size=128, crop_size=8),\n",
    "    batch_iterator_test=AugmentedBatchIterator(batch_size=128, crop_size=8),\n",
    "\n",
    "    on_epoch_finished=[SaveParams(\"net39\")],\n",
    "\n",
    "    verbose=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 6658178 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "name      size        total    cap.Y    cap.X    cov.Y    cov.X\n",
      "--------  --------  -------  -------  -------  -------  -------\n",
      "input     5x40x40      8000   100.00   100.00   100.00   100.00\n",
      "conv11    32x36x36    41472   100.00   100.00    12.50    12.50\n",
      "pool1     32x18x18    10368   100.00   100.00    12.50    12.50\n",
      "dropout1  32x18x18    10368   100.00   100.00   100.00   100.00\n",
      "conv21    64x16x16    16384   100.00   100.00   100.00   100.00\n",
      "pool2     64x8x8       4096   100.00   100.00   100.00   100.00\n",
      "dropout2  64x8x8       4096   100.00   100.00   100.00   100.00\n",
      "conv31    128x6x6      4608   100.00   100.00   100.00   100.00\n",
      "pool3     128x3x3      1152   100.00   100.00   100.00   100.00\n",
      "dropout3  128x3x3      1152   100.00   100.00   100.00   100.00\n",
      "hidden4   2048         2048   100.00   100.00   100.00   100.00\n",
      "dropout4  2048         2048   100.00   100.00   100.00   100.00\n",
      "hidden5   2048         2048   100.00   100.00   100.00   100.00\n",
      "output    2               2   100.00   100.00   100.00   100.00\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m0.76739\u001b[0m       \u001b[32m0.72180\u001b[0m      1.06316      0.28571  2.90s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<__main__.AugmentedBatchIterator object at 0x2b6ee53c9b90>,\n",
       "     batch_iterator_train=<__main__.AugmentedBatchIterator object at 0x2b6ec610a310>,\n",
       "     conv11_filter_size=(5, 5), conv11_num_filters=32,\n",
       "     conv21_filter_size=(3, 3), conv21_num_filters=64,\n",
       "     conv31_filter_size=(3, 3), conv31_num_filters=128, custom_score=None,\n",
       "     dropout1_p=0.1, dropout2_p=0.1, dropout3_p=0.5, dropout4_p=0.5,\n",
       "     hidden4_num_units=2048, hidden5_num_units=2048,\n",
       "     input_shape=(None, 5, 40, 40),\n",
       "     layers=[(u'input', <class 'lasagne.layers.input.InputLayer'>), (u'conv11', <class 'lasagne.layers.conv.Conv2DLayer'>), (u'pool1', <class 'lasagne.layers.pool.MaxPool2DLayer'>), (u'dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), (u'conv21', <class 'lasagne.layers.conv.Conv2DLayer'>), (u'pool... <class 'lasagne.layers.dense.DenseLayer'>), (u'output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=3000, more_params={},\n",
       "     objective=<function objective at 0x2b6ee441a0c8>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x2b6ee2d310c8>,\n",
       "     on_batch_finished=[],\n",
       "     on_epoch_finished=[<__main__.SaveParams object at 0x2b6ee53c9bd0>, <nolearn.lasagne.handlers.PrintLog instance at 0x2b6ee592ff80>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x2b6ee592f878>],\n",
       "     output_nonlinearity=<function softmax at 0x2b6ee1cd4848>,\n",
       "     output_num_units=2, pool1_pool_size=(2, 2), pool2_pool_size=(2, 2),\n",
       "     pool3_pool_size=(2, 2), regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x2b6ee441b110>,\n",
       "     update=<function nesterov_momentum at 0x2b6ee2d319b0>,\n",
       "     update_learning_rate=0.0001, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=2,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net39.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best valid loss: 0.721799951262\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = min([row['valid_loss'] for row in net39.train_history_])\n",
    "print(\"Best valid loss: {}\".format(best_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net39.save_params_to(\"net39.params\")\n",
    "\n",
    "import cPickle as pickle\n",
    "with open(\"net39.history\", \"wb\") as f:\n",
    "    pickle.dump(net39.train_history_, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(net):\n",
    "    train_loss = [row['train_loss'] for row in net.train_history_]\n",
    "    valid_loss = [row['valid_loss'] for row in net.train_history_]\n",
    "    plt.semilogy(train_loss, label='train loss')\n",
    "    plt.semilogy(valid_loss, label='valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAESCAYAAAAFYll6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b6ef8e79350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(net39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.426091269841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y, net39.predict_proba(X)[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
